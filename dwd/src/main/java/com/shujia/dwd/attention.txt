step1
1.在hdfs的user目录下面为每一个用户都创建一个目录(因为临时文件要存到user下面普通用户没权限)

2.先在hive创建位置融合表

3.t+1模式,从hive中导入ars参数传递的时间进行融合

4.对手机号进行加盐处理使用md5脱敏手机号

5.因为要hive元数据，所以要在集群中运行,使用dwd用户执行

6.去hdfs和hive中查看位置融合表中是否有数据

step2
1.编写shell脚本,上传到Linux,在Linux运行提交命令,注意换行符格式是LF还是CRLF(editor->CodeStyle->lineSep->unix)

2.将初始化环境的代码放到工具类中封装成抽象方法,子类重写run

3.获取当前子类的类名,day_id当做成员变量传递,spark环境当做参数传递


